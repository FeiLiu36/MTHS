'''
    {
        "algorithm": "ALGORITHM Hierarchical Motif-Consensus Optimization (HMCO)\n\n    /* PURPOSE: Unified multiscale metaheuristic that combines motif-based coarsening, adaptive consensus memory, and feasibility-aware refinement to produce high-quality feasible solutions for routing and sequencing problems */\n    \n    INPUT: problem instance (elements to order/visit, constraints, objective evaluator), feasibility checker, termination control\n    OUTPUT: a feasible permutation or set of routes/schedule and its objective value\n    \n    /* Initialization Phase */\n    Generate a diverse set of feasible seed solutions using lightweight constructive routines\n    Mine recurring high-value fragments from seeds to build an initial motif library and initialize a consensus memory of structural patterns\n    Initialize adaptive control parameters for coarsening granularity, operator selection, and intensification/diversification balance\n    \n    /* Main Processing Loop */\n    WHILE termination criteria not satisfied DO\n        Select one or more candidate solutions guided by quality, feasibility margin, and novelty signals from consensus memory\n        Construct a multilevel representation by aggregating selected motifs and consensus patterns into macro-entities to form a coarsened instance\n        Improve the coarsened instance with hierarchical search operators that mix coarse recombination and fine-grained moves\n        Expand the improved coarse solution back to the original level, invoking lightweight feasibility repair that enforces capacity or ordering constraints as needed\n        Apply focused local refinement that targets conflicts introduced by expansion and exploits consensus patterns to accelerate improvements\n        Update motif library and consensus memory by extracting robust patterns from improved solutions and prune weak motifs\n        Adapt control parameters (coarsening granularity, operator probabilities, restart triggers) based on recent progress and stagnation indicators\n        Occasionally perform targeted perturbation or restart seeded from consensus memory to escape deep local optima\n    END WHILE\n    \n    /* Post-Processing Phase */\n    Apply a final full-scale polishing pass to the best feasible solution, tighten feasibility, and return the best solution and its objective value",
        "task_scores": [
            -0.01707778896920693,
            -0.10585779894160419,
            -0.12205325295943911
        ],
        "task_programs": [
            "# our updated program here\nimport numpy as np\nimport time\nimport random\nfrom collections import Counter, defaultdict\nfrom typing import List, Tuple, Dict, Set\n\nclass TSPSolver:\n    def __init__(self, coordinates: np.ndarray, distance_matrix: np.ndarray):\n        \"\"\"\n        Initialize the TSP solver.\n\n        Args:\n            coordinates: Numpy array of shape (n, 2) containing the (x, y) coordinates of each city.\n            distance_matrix: Numpy array of shape (n, n) containing pairwise distances between cities.\n        \"\"\"\n        self.coordinates = coordinates\n        self.distance_matrix = distance_matrix\n        self.n = len(coordinates)\n\n    # --- helper methods ---\n    def tour_length(self, tour: np.ndarray) -> float:\n        n = len(tour)\n        if n == 0:\n            return 0.0\n        dist = self.distance_matrix\n        idx = tour\n        # looped tour\n        return float(np.sum(dist[idx, np.roll(idx, -1)]))\n\n    def nearest_neighbor_tour(self, start: int) -> np.ndarray:\n        n = self.n\n        unvisited = set(range(n))\n        tour = [start]\n        unvisited.remove(start)\n        cur = start\n        dist = self.distance_matrix\n        while unvisited:\n            # choose nearest with random tie breaking\n            candidates = list(unvisited)\n            dists = dist[cur, candidates]\n            min_val = dists.min()\n            ties = [candidates[i] for i, v in enumerate(dists) if abs(v - min_val) < 1e-12]\n            nxt = random.choice(ties)\n            tour.append(nxt)\n            unvisited.remove(nxt)\n            cur = nxt\n        return np.array(tour, dtype=int)\n\n    def random_greedy_insertion(self) -> np.ndarray:\n        # simple randomized insertion: start with small subtour then insert closest\n        n = self.n\n        nodes = list(range(n))\n        random.shuffle(nodes)\n        tour = nodes[:3] if n >= 3 else nodes[:]\n        remaining = nodes[3:] if n >= 3 else []\n        while remaining:\n            v = remaining.pop()\n            # find best insertion position minimizing added length\n            best_pos = 0\n            best_delta = None\n            for i in range(len(tour)):\n                a = tour[i]\n                b = tour[(i + 1) % len(tour)]\n                delta = self.distance_matrix[a, v] + self.distance_matrix[v, b] - self.distance_matrix[a, b]\n                if best_delta is None or delta < best_delta:\n                    best_delta = delta\n                    best_pos = i + 1\n            tour.insert(best_pos, v)\n        if len(tour) < n:\n            # add missing nodes if any\n            remaining = set(range(n)) - set(tour)\n            for v in remaining:\n                tour.append(v)\n        return np.array(tour, dtype=int)\n\n    def two_opt(self, tour: np.ndarray, max_passes: int = 50, candidate_edges: Set[Tuple[int,int]] = None) -> np.ndarray:\n        \"\"\"\n        Improved 2-opt variation:\n        - Uses candidate neighbor lists (from candidate_edges if provided, otherwise k nearest).\n        - Employs don't-look bits (dlb) to skip unpromising edges.\n        - Uses first-improvement on neighbor candidates.\n        - Handles circular reversals robustly and updates positions in O(len(reversed_segment)).\n        - A few other micro-optimizations and safer fallbacks.\n        \"\"\"\n        import numpy as _np\n        import random as _random\n\n        n = len(tour)\n        if n < 4:\n            return tour.copy()\n\n        dist = self.distance_matrix  # expected as a 2D numpy array\n        tour = tour.copy().astype(int)\n\n        # inverse position array: pos[node] -> index in tour\n        pos = _np.empty(n, dtype=int)\n        pos[tour] = _np.arange(n, dtype=int)\n\n        # dlb: True means \"consider (look) this position\", False means \"don't look\"\n        dlb = _np.ones(n, dtype=bool)\n\n        eps = 1e-12\n        rng = _random.random\n        randrange = _random.randrange\n\n        # Build neighbor lists: prefer candidate_edges if provided and non-empty, otherwise use k nearest\n        neighbors = [[] for _ in range(n)]\n        use_candidates = bool(candidate_edges)  # treat empty container as not provided\n        if use_candidates:\n            # candidate_edges are undirected pairs (u,v) possibly ordered arbitrarily\n            for (u, v) in candidate_edges:\n                if not (0 <= u < n and 0 <= v < n):\n                    continue\n                if v not in neighbors[u]:\n                    neighbors[u].append(v)\n                if u not in neighbors[v]:\n                    neighbors[v].append(u)\n            # for nodes with no candidates, fall back to a few nearest neighbors\n            k_fallback = min(15, n - 1)\n            for a in range(n):\n                if not neighbors[a]:\n                    if n - 1 <= k_fallback:\n                        # all nodes except self\n                        neighbors[a] = [int(x) for x in range(n) if x != a]\n                    else:\n                        idx = _np.argpartition(dist[a], k_fallback + 1)[:k_fallback + 1]\n                        # filter out self and limit to k_fallback, sort chosen by distance for better ordering\n                        idx_filtered = idx[idx != a]\n                        idx_sorted = idx_filtered[_np.argsort(dist[a][idx_filtered])]\n                        neighbors[a] = [int(x) for x in idx_sorted[:k_fallback]]\n        else:\n            # use k nearest neighbors for each node\n            k = min(20, n - 1)\n            for a in range(n):\n                if n - 1 <= k:\n                    neighbors[a] = [int(x) for x in range(n) if x != a]\n                else:\n                    idx = _np.argpartition(dist[a], k + 1)[:k + 1]\n                    idx_sorted = idx[_np.argsort(dist[a][idx])]\n                    # filter out self\n                    lst = []\n                    for c in idx_sorted:\n                        if c == a:\n                            continue\n                        lst.append(int(c))\n                        if len(lst) >= k:\n                            break\n                    neighbors[a] = lst\n\n        passes = 0\n        improved_any_pass = True\n\n        while improved_any_pass and passes < max_passes:\n            passes += 1\n            improved_any_pass = False\n\n            # Optionally randomize starting index to diversify search\n            start = randrange(n)\n            # iterate through positions in tour; don't-look bits skip \"unpromising\" spots\n            for offset in range(n):\n                i = (start + offset) % n\n                if not dlb[i]:\n                    continue\n                a = int(tour[i])\n                b_idx = (i + 1) % n\n                b = int(tour[b_idx])\n                # mark as being inspected now\n                dlb[i] = False\n                improved_this_i = False\n\n                # Candidate neighbor positions (positions in the tour corresponding to neighbor nodes)\n                nbrs = neighbors[a]\n                candidate_positions = []\n\n                if not nbrs:\n                    # fallback: sample some random positions excluding i and its immediate neighbors\n                    sample_count = min(10, n - 3)\n                    samples = set()\n                    attempts = 0\n                    while len(samples) < sample_count and attempts < sample_count * 6:\n                        jpos = randrange(n)\n                        if jpos == i or jpos == b_idx or jpos == (i - 1) % n:\n                            attempts += 1\n                            continue\n                        samples.add(jpos)\n                    candidate_positions = list(samples)\n                else:\n                    # translate neighbor node labels to positions in the current tour\n                    # Filter out trivial adjacency (same edge endpoints)\n                    for cnode in nbrs:\n                        j = int(pos[cnode])\n                        if j == i or j == b_idx or j == (i - 1) % n:\n                            continue\n                        candidate_positions.append(j)\n\n                    # occasionally mix in a couple random positions to avoid local traps\n                    if rng() < 0.05 and n > 6:\n                        added = 0\n                        attempts = 0\n                        while added < 2 and attempts < 10:\n                            rp = randrange(n)\n                            attempts += 1\n                            if rp in candidate_positions or rp == i or rp == b_idx or rp == (i - 1) % n:\n                                continue\n                            candidate_positions.append(rp)\n                            added += 1\n\n                # try first-improvement among candidate_positions\n                # We don't sort candidates here; the neighbor lists are already distance-ordered where possible\n                for j in candidate_positions:\n                    # Avoid the trivial reversal that would reconnect the endpoints in some representations\n                    if i == 0 and j == n - 1:\n                        continue\n                    c = int(tour[j])\n                    d = int(tour[(j + 1) % n])\n                    # compute 2-opt gain: remove edges (a,b) and (c,d), add (a,c) and (b,d)\n                    delta = dist[a, c] + dist[b, d] - dist[a, b] - dist[c, d]\n                    if delta < -eps:\n                        # perform 2-opt reversal between positions (i+1) .. j (circularly)\n                        if i < j:\n                            # straightforward slice reversal using numpy (creates a copy on RHS)\n                            seg = tour[i+1:j+1].copy()\n                            seg = seg[::-1]\n                            tour[i+1:j+1] = seg\n                            # update pos only for affected nodes in the reversed segment\n                            affected_nodes = tour[i+1:j+1]\n                            pos[affected_nodes] = _np.arange(i+1, j+1, dtype=int)\n                        else:\n                            # wrap-around segment: i+1..n-1 and 0..j\n                            tail = tour[i+1:].copy()\n                            head = tour[:j+1].copy()\n                            segment = _np.concatenate((tail, head))[::-1]\n                            # split reversed segment back into tail and head parts\n                            tail_len = n - (i + 1)\n                            tail_part = segment[:tail_len]\n                            head_part = segment[tail_len:]\n                            if tail_len > 0:\n                                tour[i+1:] = tail_part\n                            tour[:j+1] = head_part\n                            # update pos only for affected nodes (new positions of reversed nodes)\n                            if tail_len > 0:\n                                affected_nodes_tail = tour[i+1:]\n                                pos[affected_nodes_tail] = _np.arange(i+1, n, dtype=int)\n                            affected_nodes_head = tour[:j+1]\n                            pos[affected_nodes_head] = _np.arange(0, j+1, dtype=int)\n\n                        improved_this_i = True\n                        improved_any_pass = True\n\n                        # Mark don't-look bits conservatively: re-enable area around change\n                        dlb[i] = True\n                        dlb[(i + 1) % n] = True\n                        dlb[j] = True\n                        dlb[(j + 1) % n] = True\n\n                        # Also re-enable neighbors of the moved nodes to allow further improvements\n                        # (a small neighborhood helps to propagate beneficial moves)\n                        dlb[(i - 1) % n] = True\n                        dlb[(j - 1) % n] = True\n\n                        # break on first improvement for this i\n                        break\n\n                if improved_this_i:\n                    # continue scanning other i's in the same pass (we don't restart the pass immediately)\n                    continue\n                else:\n                    # no improvement for this i; keep it flagged as not to look until neighbors change\n                    dlb[i] = False\n\n            # end of pass loop\n        return tour\n\n    def generate_seed_solutions(self, k: int = 10) -> List[np.ndarray]:\n        seeds = []\n        n = self.n\n        # use variety: nearest neighbor from random starts, random greedy insertion, random shuffles\n        starts = list(range(n))\n        random.shuffle(starts)\n        for i in range(min(k // 2, n)):\n            seeds.append(self.nearest_neighbor_tour(starts[i]))\n        for i in range(min(k // 4, n)):\n            seeds.append(self.random_greedy_insertion())\n        # fill with random permutations\n        while len(seeds) < k:\n            tour = np.arange(n)\n            np.random.shuffle(tour)\n            seeds.append(tour.copy())\n        # local improve seeds with short 2-opt passes\n        improved = []\n        for t in seeds:\n            improved.append(self.two_opt(t, max_passes=10))\n        # deduplicate by tuple\n        uniq = []\n        seen = set()\n        for t in improved:\n            key = tuple(t.tolist())\n            if key not in seen:\n                seen.add(key)\n                uniq.append(t)\n        return uniq\n\n    def extract_motifs_from_seeds(self, seeds: List[np.ndarray], min_len: int = 3, max_len: int = 8, top_k: int = 50) -> Tuple[Dict[Tuple[int,...], int], Dict[Tuple[int,...], int]]:\n        # count directed motifs and undirected (canonical) motifs across seeds\n        motif_count = Counter()\n        motif_orient_count = Counter()\n        for tour in seeds:\n            n = len(tour)\n            for L in range(min_len, min(max_len, n) + 1):\n                for i in range(n):\n                    seg = tuple(tour[i:i+L]) if i+L <= n else tuple(np.concatenate((tour[i:], tour[:(i+L)%n])))\n                    # canonical key treat reversed same for popularity counting\n                    rev = tuple(seg[::-1])\n                    canon = seg if seg <= rev else rev\n                    motif_count[canon] += 1\n                    motif_orient_count[seg] += 1\n        # select top_k canonical motifs\n        common = dict()\n        orient = dict()\n        for motif, cnt in motif_count.most_common(top_k):\n            common[motif] = cnt\n            # store the most frequent orientation for this canonical motif from orient counts\n            # find among motif and reversed which orientation occurs more in orient counts\n            rev = motif[::-1]\n            orient_count_m = sum(v for k, v in motif_orient_count.items() if tuple(k) == motif)\n            # we just choose orientation by comparing summed occurrences of motif vs reversed\n            orient[motif] = motif if motif_orient_count.get(motif, 0) >= motif_orient_count.get(rev, 0) else rev\n        return common, orient\n\n    def select_non_overlapping_motifs(self, motif_counts: Dict[Tuple[int,...], int], cover_limit: float = 0.4) -> List[Tuple[int,...]]:\n        # Greedily select motifs by count, avoid overlapping nodes, until cover limit reached\n        selected = []\n        covered = set()\n        total_nodes = self.n\n        motifs_sorted = sorted(motif_counts.items(), key=lambda x: -x[1])\n        for motif, cnt in motifs_sorted:\n            if any(v in covered for v in motif):\n                continue\n            selected.append(motif)\n            covered.update(motif)\n            if len(covered) / total_nodes >= cover_limit:\n                break\n        return selected\n\n    def build_coarsened_instance(self, macros: List[Tuple[int,...]]) -> Tuple[List[List[int]], np.ndarray]:\n        # macros: list of node sequences (tuples). Build groups: each macro is group, remaining nodes as singletons.\n        assigned = set()\n        groups = []\n        for m in macros:\n            groups.append(list(m))\n            assigned.update(m)\n        for v in range(self.n):\n            if v not in assigned:\n                groups.append([v])\n        # compute reduced distance matrix between groups as average distance\n        k = len(groups)\n        red = np.zeros((k, k))\n        dist = self.distance_matrix\n        for i in range(k):\n            for j in range(i+1, k):\n                gi = groups[i]\n                gj = groups[j]\n                # average pairwise distances\n                mat = dist[np.ix_(gi, gj)]\n                val = float(np.mean(mat))\n                red[i, j] = val\n                red[j, i] = val\n        return groups, red\n\n    def solve_reduced_tsp(self, red_dist: np.ndarray) -> List[int]:\n        k = red_dist.shape[0]\n        if k == 0:\n            return []\n        # greedy nearest neighbor from random starts + 2-opt on reduced\n        best = None\n        best_len = float('inf')\n        for _ in range(min(6, k)):\n            start = random.randrange(k)\n            unv = set(range(k))\n            tour = [start]\n            unv.remove(start)\n            cur = start\n            while unv:\n                candidates = list(unv)\n                dists = red_dist[cur, candidates]\n                min_val = dists.min()\n                ties = [candidates[i] for i, v in enumerate(dists) if abs(v - min_val) < 1e-12]\n                nxt = random.choice(ties)\n                tour.append(nxt)\n                unv.remove(nxt)\n                cur = nxt\n            # simple 2-opt on reduced matrix\n            tour_arr = np.array(tour, dtype=int)\n            # small 2-opt implementation for reduced\n            improved = True\n            passes = 0\n            while improved and passes < 10:\n                improved = False\n                passes += 1\n                for i in range(0, k - 2):\n                    for j in range(i+2, k):\n                        if i == 0 and j == k - 1:\n                            continue\n                        a = tour_arr[i]\n                        b = tour_arr[i+1]\n                        c = tour_arr[j]\n                        d = tour_arr[(j+1) % k]\n                        delta = red_dist[a, c] + red_dist[b, d] - red_dist[a, b] - red_dist[c, d]\n                        if delta < -1e-12:\n                            tour_arr[i+1:j+1] = tour_arr[i+1:j+1][::-1]\n                            improved = True\n                            break\n                    if improved:\n                        break\n            # compute length\n            L = sum(red_dist[tour_arr[i], tour_arr[(i+1)%k]] for i in range(k))\n            if L < best_len:\n                best_len = L\n                best = tour_arr.tolist()\n        return best if best is not None else list(range(k))\n\n    def expand_macro_tour(self, macro_groups: List[List[int]], macro_tour: List[int], orient_map: Dict[Tuple[int,...], Tuple[int,...]]) -> np.ndarray:\n        # Expand macro tour back to full node tour. For groups that are original singletons, return the single node.\n        full = []\n        for idx in macro_tour:\n            grp = macro_groups[idx]\n            if len(grp) == 1:\n                full.extend(grp)\n            else:\n                tup = tuple(grp)\n                # try to preserve original motif orientation if available, else keep as stored\n                # orient_map keys are canonical motifs mapping to preferred orientation\n                # find canonical key\n                rev = tuple(grp[::-1])\n                canon = tup if tup <= rev else rev\n                if canon in orient_map:\n                    preferred = orient_map[canon]\n                    # preferred might be canonical or reversed tuple; ensure orientation matches nodes set\n                    if set(preferred) == set(grp):\n                        seq = list(preferred)\n                        full.extend(seq)\n                    else:\n                        full.extend(grp)\n                else:\n                    full.extend(grp)\n        # ensure it's a permutation; if duplicates occur because overlapping selection possible (shouldn't), remove duplicates preserving order then append missing\n        seen = set()\n        clean = []\n        for v in full:\n            if v not in seen:\n                clean.append(v)\n                seen.add(v)\n        for v in range(self.n):\n            if v not in seen:\n                clean.append(v)\n        return np.array(clean, dtype=int)\n\n    def build_consensus_edges(self, seeds: List[np.ndarray]) -> Counter:\n        # count undirected edges across seeds\n        edge_cnt = Counter()\n        for tour in seeds:\n            n = len(tour)\n            for i in range(n):\n                a = int(tour[i])\n                b = int(tour[(i+1) % n])\n                key = (min(a, b), max(a, b))\n                edge_cnt[key] += 1\n        return edge_cnt\n\n    # --- main solver implementing HMCO-like logic ---\n    def solve(self, max_iters: int = 200, time_limit: float = 10.0) -> np.ndarray:\n        \"\"\"\n        Solve the Traveling Salesman Problem (TSP).\n\n        Returns:\n            A numpy array of shape (n,) containing a permutation of integers\n            [0, 1, ..., n-1] representing the order in which the cities are visited.\n        \"\"\"\n        n = self.n\n        start_time = time.time()\n        # Initialization Phase\n        seeds = self.generate_seed_solutions(k=min(20, max(6, n)))\n        # consensus: motifs and edges\n        motif_counts, motif_orients = self.extract_motifs_from_seeds(seeds, min_len=3, max_len=min(8, max(3, n//10)), top_k=80)\n        edge_counts = self.build_consensus_edges(seeds)\n\n        # baseline best solution\n        best = min(seeds, key=self.tour_length)\n        best_val = self.tour_length(best)\n        current_meta = best.copy()\n        current_val = best_val\n\n        # adaptive params\n        cover_limit = 0.35  # fraction of nodes to coarsen\n        stagnation = 0\n        restart_trigger = 20\n        iter_no = 0\n\n        # main loop\n        while iter_no < max_iters and (time.time() - start_time) < time_limit:\n            iter_no += 1\n\n            # Select candidate solutions (choose based on quality and novelty)\n            # pick from seeds and best\n            candidates = [best]\n            if seeds:\n                candidates.append(random.choice(seeds))\n            # build consensus signals\n            # select motifs to coarsen\n            selected_macros = self.select_non_overlapping_motifs(motif_counts, cover_limit=cover_limit)\n            # Build coarsened instance\n            macro_groups, red = self.build_coarsened_instance(selected_macros)\n            # Solve coarsened instance\n            macro_tour = self.solve_reduced_tsp(red)\n            # Expand solution\n            expanded = self.expand_macro_tour(macro_groups, macro_tour, motif_orients)\n            # lightweight feasibility repair: for TSP nothing special; but ensure it's a permutation\n            if len(set(expanded)) != n:\n                # fallback: reorder to include all nodes uniquely - keep order then append missing\n                seen = set()\n                clean = []\n                for v in expanded:\n                    if v not in seen:\n                        clean.append(v)\n                        seen.add(v)\n                for v in range(n):\n                    if v not in seen:\n                        clean.append(v)\n                expanded = np.array(clean, dtype=int)\n            # Focused local refinement\n            # build candidate_edges (top consensus edges)\n            top_edges = set([e for e, _ in edge_counts.most_common(max(10, n//5))])\n            refined = self.two_opt(expanded, max_passes=60, candidate_edges=top_edges)\n            refined_val = self.tour_length(refined)\n\n            # Update motif library and consensus memory\n            # extract motifs from refined and add counts\n            new_motif_counts, new_orients = self.extract_motifs_from_seeds([refined], min_len=3, max_len=min(8, max(3, n//10)), top_k=80)\n            # merge counts\n            for k_m, v in new_motif_counts.items():\n                motif_counts[k_m] = motif_counts.get(k_m, 0) + v\n            for k_o, v in new_orients.items():\n                motif_orients[k_o] = v  # overwrite with latest orientation (keeps preference)\n            # update edge counts\n            new_edges = self.build_consensus_edges([refined])\n            for e, c in new_edges.items():\n                edge_counts[e] += c\n\n            # Adapt control parameters\n            if refined_val < best_val - 1e-12:\n                best = refined.copy()\n                best_val = refined_val\n                stagnation = 0\n                # intensify: reduce cover to focus\n                cover_limit = max(0.15, cover_limit * 0.95)\n            else:\n                stagnation += 1\n                cover_limit = min(0.6, cover_limit * 1.03)\n\n            # occasionally perturb / restart if stagnation\n            if stagnation >= restart_trigger:\n                stagnation = 0\n                # targeted perturbation seeded from consensus: build tour from top edges greedily\n                # Try to construct a tour preferring consensus edges\n                pref_tour = self.build_tour_from_consensus(edge_counts)\n                pref_tour = self.two_opt(pref_tour, max_passes=40)\n                pref_val = self.tour_length(pref_tour)\n                if pref_val < best_val:\n                    best = pref_tour.copy()\n                    best_val = pref_val\n                # also random restart small hillclimb\n                r = np.arange(n)\n                np.random.shuffle(r)\n                r = self.two_opt(r, max_passes=30)\n                r_val = self.tour_length(r)\n                if r_val < best_val:\n                    best = r.copy()\n                    best_val = r_val\n\n            # maintain seed pool: keep some best solutions\n            seeds.append(refined.copy())\n            # keep only top K seeds by quality\n            seeds = sorted(seeds, key=self.tour_length)[:min(30, max(6, len(seeds)))]\n            # prune motif library (remove weak motifs)\n            motif_counts = Counter({k:v for k,v in motif_counts.items() if v >= max(1, iter_no//3)})\n            # small time check\n            if (time.time() - start_time) >= time_limit:\n                break\n\n        # Post-processing polishing pass\n        final = self.two_opt(best.copy(), max_passes=200)\n        final_val = self.tour_length(final)\n        if final_val < best_val:\n            best = final\n            best_val = final_val\n\n        return np.array(best, dtype=int)\n\n    def build_tour_from_consensus(self, edge_counts: Counter) -> np.ndarray:\n        # attempt greedy path building from high-count edges, then fill remaining nodes\n        n = self.n\n        # build adjacency preference from edge counts\n        neigh = defaultdict(list)\n        for (a, b), c in edge_counts.items():\n            neigh[a].append((b, c))\n            neigh[b].append((a, c))\n        # start from node with highest degree sum\n        score = {v: sum(w for _, w in neigh[v]) for v in range(n)}\n        start = max(range(n), key=lambda x: score.get(x, 0))\n        tour = [start]\n        used = {start}\n        while len(tour) < n:\n            cur = tour[-1]\n            candidates = [(v, w) for v, w in neigh.get(cur, []) if v not in used]\n            if candidates:\n                # pick highest weight, tie break by distance closeness\n                candidates.sort(key=lambda x: (-x[1], self.distance_matrix[cur, x[0]]))\n                nxt = candidates[0][0]\n            else:\n                # pick nearest unused\n                unused = [v for v in range(n) if v not in used]\n                dists = self.distance_matrix[cur, unused]\n                idx = int(np.argmin(dists))\n                nxt = unused[idx]\n            tour.append(nxt)\n            used.add(nxt)\n        return np.array(tour, dtype=int)",
            "# our updated program here\nimport numpy as np\nimport random\nimport time\nfrom collections import defaultdict, Counter\nimport math\nimport copy\n\nclass CVRPSolver:\n    def __init__(self, coordinates: np.ndarray, distance_matrix: np.ndarray, demands: list, vehicle_capacity: int):\n        \"\"\"\n        Initialize the CVRP solver.\n\n        Args:\n            coordinates: Numpy array of shape (n, 2) containing the (x, y) coordinates of each node, including the depot.\n            distance_matrix: Numpy array of shape (n, n) containing pairwise distances between nodes.\n            demands: List of integers representing the demand of each node (first node is typically the depot with zero demand).\n            vehicle_capacity: Integer representing the maximum capacity of each vehicle.\n        \"\"\"\n        self.coordinates = coordinates\n        self.distance_matrix = distance_matrix\n        self.demands = demands\n        self.n = len(coordinates)\n        self.vehicle_capacity = vehicle_capacity\n\n        # Consensus memory and motif library\n        self.edge_counts = Counter()  # counts of directed edges (u,v)\n        self.motif_counts = Counter()  # counts of motifs (tuples)\n        self.motif_quality = {}  # average quality (cost saving) observed for motif\n        self.random = random.Random(12345)\n\n        # Adaptive control parameters\n        self.coarsen_granularity = 0.15  # fraction of nodes to try to compress into macros\n        self.operator_probs = {'two_opt': 0.5, 'relocate': 0.3, 'swap': 0.2}\n        self.restart_trigger = 50  # iterations without improvement triggers stronger perturbation\n        self.max_no_improve = 100\n\n    # ------------------ Utilities ------------------\n\n    def routes_from_solution(self, solution):\n        routes = []\n        cur = []\n        for v in solution:\n            if v == 0:\n                if cur:\n                    routes.append(cur)\n                    cur = []\n            else:\n                cur.append(v)\n        if cur:\n            routes.append(cur)\n        return routes\n\n    def solution_from_routes(self, routes):\n        sol = []\n        for r in routes:\n            sol.append(0)\n            sol.extend(r)\n        sol.append(0)\n        return sol\n\n    def route_cost(self, route):\n        # route: list of customers excluding depot\n        if not route:\n            return 0.0\n        cost = 0.0\n        prev = 0\n        for v in route:\n            cost += self.distance_matrix[prev, v]\n            prev = v\n        cost += self.distance_matrix[prev, 0]\n        return cost\n\n    def total_cost(self, routes):\n        return sum(self.route_cost(r) for r in routes)\n\n    def route_demand(self, route):\n        return sum(self.demands[v] for v in route)\n\n    def is_feasible_routes(self, routes):\n        visited = set()\n        for r in routes:\n            if self.route_demand(r) > self.vehicle_capacity:\n                return False\n            for v in r:\n                if v == 0:\n                    return False\n                if v in visited:\n                    return False\n                visited.add(v)\n        return visited == set(range(1, self.n))\n\n    # ------------------ Seed generation ------------------\n\n    def generate_seed_greedy_randomized(self, greediness=0.2):\n        # Greedy randomized nearest neighbor with capacity\n        customers = list(range(1, self.n))\n        self.random.shuffle(customers)\n        unvisited = set(customers)\n        routes = []\n        while unvisited:\n            cur = []\n            cap = 0\n            cur_node = 0\n            while True:\n                candidates = []\n                for v in unvisited:\n                    if cap + self.demands[v] <= self.vehicle_capacity:\n                        dist = self.distance_matrix[cur_node, v]\n                        candidates.append((dist, v))\n                if not candidates:\n                    break\n                candidates.sort()\n                k = max(1, int(len(candidates) * greediness))\n                idx = self.random.randint(0, k-1)\n                _, chosen = candidates[idx]\n                cur.append(chosen)\n                unvisited.remove(chosen)\n                cap += self.demands[chosen]\n                cur_node = chosen\n            routes.append(cur)\n        return routes\n\n    def generate_seeds(self, num_seeds=20):\n        seeds = []\n        for i in range(num_seeds):\n            g = 0.05 + 0.4 * (i / max(1, num_seeds - 1))\n            seed = self.generate_seed_greedy_randomized(greediness=g)\n            seeds.append(seed)\n        return seeds\n\n    # ------------------ Motif mining & consensus ------------------\n\n    def mine_motifs_from_seeds(self, seeds, min_len=2, max_len=4, top_k=50):\n        counts = Counter()\n        quality = defaultdict(list)\n        for routes in seeds:\n            for r in routes:\n                L = len(r)\n                for l in range(min_len, max_len + 1):\n                    for i in range(0, L - l + 1):\n                        motif = tuple(r[i:i + l])\n                        counts[motif] += 1\n                        # approximate quality: sum of internal short edges vs separate visits\n                        internal_cost = 0.0\n                        prev = 0\n                        for v in motif:\n                            internal_cost += self.distance_matrix[prev, v]\n                            prev = v\n                        internal_cost += self.distance_matrix[prev, 0]\n                        separate_cost = 0.0\n                        for v in motif:\n                            separate_cost += (self.distance_matrix[0, v] + self.distance_matrix[v, 0])\n                        quality[motif].append(separate_cost - internal_cost)\n        # build motif library with scores\n        motif_list = []\n        for m, c in counts.items():\n            avg_q = np.mean(quality[m]) if quality[m] else 0.0\n            motif_list.append((m, c, avg_q))\n        motif_list.sort(key=lambda x: (x[1], x[2]), reverse=True)\n        selected = motif_list[:top_k]\n        self.motif_counts = Counter({m: c for (m, c, q) in selected})\n        self.motif_quality = {m: q for (m, c, q) in selected}\n\n    def update_consensus_from_solution(self, routes):\n        for r in routes:\n            prev = 0\n            for v in r:\n                self.edge_counts[(prev, v)] += 1\n                prev = v\n            self.edge_counts[(prev, 0)] += 1\n            # motifs\n            L = len(r)\n            for l in range(2, 5):\n                for i in range(0, L - l + 1):\n                    motif = tuple(r[i:i + l])\n                    self.motif_counts[motif] += 1\n\n    # ------------------ Coarsening ------------------\n\n    def build_coarse_instance(self, routes, target_fraction=None):\n        # Replace frequent motifs in routes greedily to form macros\n        if target_fraction is None:\n            target_fraction = self.coarsen_granularity\n        # select motifs to use\n        motifs = [m for m in self.motif_counts if len(m) <= 5]\n        motifs.sort(key=lambda m: (self.motif_counts[m], self.motif_quality.get(m, 0.0)), reverse=True)\n        max_macros = max(1, int((self.n - 1) * target_fraction))\n        used_count = 0\n        macro_map = {}  # macro_id -> tuple(nodes)\n        macro_contents = []\n        next_macro_id = self.n  # use ids beyond original node ids\n        coarse_routes = []\n        # For each route, replace motif occurrences non-overlapping greedily\n        for r in routes:\n            i = 0\n            new_r = []\n            while i < len(r):\n                placed = False\n                for m in motifs:\n                    L = len(m)\n                    if i + L <= len(r) and tuple(r[i:i + L]) == m and used_count < max_macros:\n                        macro_id = next_macro_id\n                        if macro_id not in macro_map:\n                            macro_map[macro_id] = m\n                            macro_contents.append(m)\n                            next_macro_id += 1\n                        new_r.append(macro_id)\n                        i += L\n                        used_count += 1\n                        placed = True\n                        break\n                if not placed:\n                    new_r.append(r[i])\n                    i += 1\n            coarse_routes.append(new_r)\n        # Build coarse demands and coordinates\n        coarse_nodes = set()\n        for r in coarse_routes:\n            coarse_nodes.update(r)\n        coarse_nodes = [0] + sorted([v for v in coarse_nodes if v != 0])\n        index_map = {v: i for i, v in enumerate(coarse_nodes)}\n        m = len(coarse_nodes)\n        coarse_coords = np.zeros((m, 2))\n        coarse_demands = [0] * m\n        # For original nodes map directly\n        for v in coarse_nodes:\n            idx = index_map[v]\n            if v < self.n:\n                coarse_coords[idx] = self.coordinates[v]\n                coarse_demands[idx] = self.demands[v]\n            else:\n                seq = macro_map[v]\n                coords = np.array([self.coordinates[u] for u in seq])\n                weights = np.array([self.demands[u] + 1.0 for u in seq])\n                coarse_coords[idx] = np.average(coords, axis=0, weights=weights)\n                coarse_demands[idx] = sum(self.demands[u] for u in seq)\n        # coarse distance matrix\n        coarse_dist = np.zeros((m, m))\n        for i in range(m):\n            for j in range(m):\n                if i == j:\n                    coarse_dist[i, j] = 0.0\n                else:\n                    coarse_dist[i, j] = np.linalg.norm(coarse_coords[i] - coarse_coords[j])\n        # Build coarse routes with new indices\n        coarse_routes_idx = []\n        for r in coarse_routes:\n            cr = [index_map[v] for v in r]\n            coarse_routes_idx.append(cr)\n        coarse_data = {\n            'routes': coarse_routes_idx,\n            'coords': coarse_coords,\n            'dist': coarse_dist,\n            'demands': coarse_demands,\n            'index_map': index_map,\n            'reverse_map': {v: v if v < self.n else v for v in coarse_nodes},\n            'macro_map': macro_map\n        }\n        return coarse_data\n\n    # ------------------ Coarse improvement (small SA/local search) ------------------\n\n    def improve_coarse(self, coarse_data, iter_limit=200):\n        routes = coarse_data['routes']\n        dist = coarse_data['dist']\n        demands = coarse_data['demands']\n        cap = self.vehicle_capacity\n        # simple neighborhood: relocate single node between routes, swap nodes\n        best_routes = [list(r) for r in routes]\n        best_cost = self.coarse_total_cost(best_routes, dist)\n        no_improve = 0\n        T0 = 1.0\n        for it in range(iter_limit):\n            T = T0 * (0.995 ** it)\n            # choose operation\n            if self.random.random() < 0.5:\n                # relocate random node to random route & position\n                from_r = self.random.randrange(len(best_routes))\n                if not best_routes[from_r]:\n                    continue\n                i = self.random.randrange(len(best_routes[from_r]))\n                node = best_routes[from_r].pop(i)\n                to_r = self.random.randrange(len(best_routes))\n                insert_pos = self.random.randrange(len(best_routes[to_r]) + 1)\n                best_routes[to_r].insert(insert_pos, node)\n                # check capacity\n                if self.coarse_route_demand(best_routes[from_r], demands) > cap or self.coarse_route_demand(best_routes[to_r], demands) > cap:\n                    # revert\n                    best_routes[to_r].pop(insert_pos)\n                    best_routes[from_r].insert(i, node)\n                    continue\n                new_cost = self.coarse_total_cost(best_routes, dist)\n                delta = new_cost - best_cost\n                if delta < 0 or math.exp(-delta / max(1e-8, T)) > self.random.random():\n                    best_cost = new_cost\n                    no_improve = 0\n                else:\n                    # revert\n                    best_routes[to_r].pop(insert_pos)\n                    best_routes[from_r].insert(i, node)\n                    no_improve += 1\n            else:\n                # swap nodes between routes\n                ra = self.random.randrange(len(best_routes))\n                rb = self.random.randrange(len(best_routes))\n                if ra == rb or not best_routes[ra] or not best_routes[rb]:\n                    continue\n                ia = self.random.randrange(len(best_routes[ra]))\n                ib = self.random.randrange(len(best_routes[rb]))\n                best_routes[ra][ia], best_routes[rb][ib] = best_routes[rb][ib], best_routes[ra][ia]\n                if self.coarse_route_demand(best_routes[ra], demands) > cap or self.coarse_route_demand(best_routes[rb], demands) > cap:\n                    # revert\n                    best_routes[ra][ia], best_routes[rb][ib] = best_routes[rb][ib], best_routes[ra][ia]\n                    continue\n                new_cost = self.coarse_total_cost(best_routes, dist)\n                delta = new_cost - best_cost\n                if delta < 0 or math.exp(-delta / max(1e-8, T)) > self.random.random():\n                    best_cost = new_cost\n                    no_improve = 0\n                else:\n                    best_routes[ra][ia], best_routes[rb][ib] = best_routes[rb][ib], best_routes[ra][ia]\n                    no_improve += 1\n            if no_improve > 40:\n                break\n        return best_routes\n\n    def coarse_route_demand(self, route, demands):\n        return sum(demands[v] for v in route)\n\n    def coarse_total_cost(self, routes, dist):\n        cost = 0.0\n        for r in routes:\n            prev = 0\n            for v in r:\n                cost += dist[prev, v]\n                prev = v\n            cost += dist[prev, 0]\n        return cost\n\n    # ------------------ Expansion & repair ------------------\n\n    def expand_coarse_solution(self, coarse_routes, coarse_data):\n        # coarse_data has macro_map mapping macro_id (original id > n-1) to sequences\n        index_map = coarse_data['index_map']\n        reverse_map = {idx: node for node, idx in index_map.items()}\n        # Build expanded routes by replacing macro indices\n        expanded = []\n        for r in coarse_routes:\n            new_r = []\n            for idx in r:\n                node = reverse_map[idx]\n                if node < self.n:\n                    new_r.append(node)\n                else:\n                    seq = coarse_data['macro_map'].get(node, ())\n                    new_r.extend(seq)\n            expanded.append(new_r)\n        # repair: ensure each customer visited once; remove duplicates or missing\n        all_nodes = []\n        for r in expanded:\n            all_nodes.extend(r)\n        counts = Counter(all_nodes)\n        # Remove duplicates keeping first occurrence\n        seen = set()\n        for i, r in enumerate(expanded):\n            newr = []\n            for v in r:\n                if v not in seen:\n                    newr.append(v)\n                    seen.add(v)\n            expanded[i] = newr\n        missing = [v for v in range(1, self.n) if v not in seen]\n        # Insert missing greedily to best insertion position\n        for v in missing:\n            best_gain = None\n            best_pos = None\n            best_route = None\n            for i, r in enumerate(expanded):\n                if self.route_demand(r) + self.demands[v] > self.vehicle_capacity:\n                    continue\n                for p in range(len(r) + 1):\n                    # compute marginal increase\n                    prev = 0 if p == 0 else r[p - 1]\n                    nxt = 0 if p == len(r) else r[p]\n                    gain = self.distance_matrix[prev, v] + self.distance_matrix[v, nxt] - self.distance_matrix[prev, nxt]\n                    if best_gain is None or gain < best_gain:\n                        best_gain = gain\n                        best_pos = p\n                        best_route = i\n            if best_route is None:\n                # create new route if needed\n                expanded.append([v])\n            else:\n                expanded[best_route].insert(best_pos, v)\n        # ensure capacity by splitting heavy routes\n        repaired = []\n        for r in expanded:\n            cur = []\n            cap = 0\n            for v in r:\n                if cap + self.demands[v] > self.vehicle_capacity:\n                    repaired.append(cur)\n                    cur = [v]\n                    cap = self.demands[v]\n                else:\n                    cur.append(v)\n                    cap += self.demands[v]\n            if cur:\n                repaired.append(cur)\n        return repaired\n\n    # ------------------ Local refinement ------------------\n\n    def local_refine(self, routes, iter_limit=200):\n        best_routes = [list(r) for r in routes]\n        best_cost = self.total_cost(best_routes)\n        no_improve = 0\n        for it in range(iter_limit):\n            if self.random.random() < self.operator_probs['two_opt']:\n                improved = False\n                # 2-opt within routes\n                for i, r in enumerate(best_routes):\n                    new_r, changed = self.two_opt_single_route(r)\n                    if changed:\n                        best_routes[i] = new_r\n                        improved = True\n                        break\n                if improved:\n                    new_cost = self.total_cost(best_routes)\n                    if new_cost < best_cost:\n                        best_cost = new_cost\n                        no_improve = 0\n                    else:\n                        no_improve += 1\n                else:\n                    no_improve += 1\n            elif self.random.random() < self.operator_probs['relocate'] + self.operator_probs['two_opt']:\n                # relocate\n                success = self.relax_relocate(best_routes)\n                if success:\n                    new_cost = self.total_cost(best_routes)\n                    if new_cost < best_cost:\n                        best_cost = new_cost\n                        no_improve = 0\n                    else:\n                        no_improve += 1\n                else:\n                    no_improve += 1\n            else:\n                # swap\n                success = self.swap_between_routes(best_routes)\n                if success:\n                    new_cost = self.total_cost(best_routes)\n                    if new_cost < best_cost:\n                        best_cost = new_cost\n                        no_improve = 0\n                    else:\n                        no_improve += 1\n                else:\n                    no_improve += 1\n            if no_improve > 40:\n                break\n        # final pass: attempt reinsertion improvements\n        self.improve_by_reinsertion(best_routes, attempts=200)\n        return best_routes\n\n    # Replaced original two_opt_single_route with optimized implementation adapted for open routes with depot endpoints.\n    def two_opt_single_route(self, route, max_passes: int = 50, candidate_edges=None):\n        \"\"\"\n        Improved 2-opt for a single route (open route starting and ending at depot).\n        Returns (new_route_list, changed_flag).\n\n        This adapts a don't-look-bits style 2-opt to the open-route case where the depot (0)\n        is implicitly connected before the first customer and after the last customer.\n        \"\"\"\n        n = len(route)\n        if n < 4:\n            return route, False\n\n        dist = self.distance_matrix  # local reference\n        tour = np.array(route, dtype=int)  # work on a numpy copy for fast slice ops\n        dlb = np.ones(n, dtype=bool)  # don't-look bits\n        rng = self.random.random\n        eps = 1e-12\n\n        def edge_key(u, v):\n            return (u, v) if u <= v else (v, u)\n\n        passes = 0\n        improved_any = False\n\n        # iterate until no improvement or max passes reached\n        while passes < max_passes:\n            passes += 1\n            any_improved_this_pass = False\n\n            # scan edges (i, i+1) where i in 0..n-2 (edge between node i and i+1)\n            for i in range(n - 1):\n                if not dlb[i]:\n                    continue\n\n                a = int(tour[i])\n                b = int(tour[i + 1])\n                dlb[i] = False\n                dist_ab = dist[a, b]\n                e1 = None\n                if candidate_edges is not None:\n                    e1 = edge_key(a, b)\n\n                improved = False\n                j = i + 1\n                # consider j from i+1 .. n-1 (inclusive). When j == n-1, d is depot 0.\n                while j < n:\n                    # skip the trivial adjacent case when j == i+1? We allow reversing single element (i+1..i+1) which does nothing.\n                    c = int(tour[j])\n                    d = int(tour[j + 1]) if (j + 1) < n else 0\n\n                    # candidate pruning\n                    if candidate_edges is not None:\n                        e2 = edge_key(c, d)\n                        if (e1 not in candidate_edges) and (e2 not in candidate_edges):\n                            # occasional escape\n                            if rng() > 0.05:\n                                j += 1\n                                continue\n\n                    # compute delta for replacing edges (a,b)+(c,d) with (a,c)+(b,d)\n                    # Note: if d == 0, dist[c, d] is dist[c,0], and dist[b,d] uses dist[b,0].\n                    delta = dist[a, c] + dist[b, d] - dist_ab - dist[c, d]\n\n                    if delta < -eps:\n                        # perform reversal of segment (i+1 .. j) inclusive\n                        start = i + 1\n                        end = j + 1  # slice end is exclusive\n                        tour[start:end] = tour[start:end][::-1]\n\n                        # mark don't-look bits around changed edges\n                        dlb[i] = True\n                        if i + 1 < n:\n                            dlb[i + 1] = True\n                        dlb[j] = True\n                        if j + 1 < n:\n                            dlb[j + 1] = True\n\n                        improved = True\n                        any_improved_this_pass = True\n                        improved_any = True\n                        break  # restart scanning after improvement\n\n                    j += 1\n\n                if improved:\n                    # restart scan from beginning (typical don't-look restart)\n                    break\n\n            if not any_improved_this_pass:\n                break\n\n        return list(tour), improved_any\n\n    def relax_relocate(self, routes):\n        # try relocate any single customer to any route where capacity allows and improves cost\n        best_delta = 0\n        move = None\n        for i, r in enumerate(routes):\n            for pi, v in enumerate(r):\n                for j, r2 in enumerate(routes):\n                    if i == j:\n                        continue\n                    if self.route_demand(r2) + self.demands[v] > self.vehicle_capacity:\n                        continue\n                    for pj in range(len(r2) + 1):\n                        # cost if remove from r and insert into r2 at pj\n                        orig_cost = self.route_cost(r) + self.route_cost(r2)\n                        nr = r[:pi] + r[pi + 1:]\n                        nr2 = r2[:pj] + [v] + r2[pj:]\n                        new_cost = self.route_cost(nr) + self.route_cost(nr2)\n                        delta = new_cost - orig_cost\n                        if delta < best_delta:\n                            best_delta = delta\n                            move = (i, pi, j, pj)\n        if move:\n            i, pi, j, pj = move\n            v = routes[i].pop(pi)\n            routes[j].insert(pj, v)\n            # remove empty route\n            if not routes[i]:\n                routes.pop(i)\n            return True\n        return False\n\n    def swap_between_routes(self, routes):\n        best_delta = 0\n        move = None\n        for i in range(len(routes)):\n            for j in range(i + 1, len(routes)):\n                r1, r2 = routes[i], routes[j]\n                for a in range(len(r1)):\n                    for b in range(len(r2)):\n                        new_r1 = r1[:a] + [r2[b]] + r1[a + 1:]\n                        new_r2 = r2[:b] + [r1[a]] + r2[b + 1:]\n                        if self.route_demand(new_r1) > self.vehicle_capacity or self.route_demand(new_r2) > self.vehicle_capacity:\n                            continue\n                        delta = self.route_cost(new_r1) + self.route_cost(new_r2) - (self.route_cost(r1) + self.route_cost(r2))\n                        if delta < best_delta:\n                            best_delta = delta\n                            move = (i, a, j, b)\n        if move:\n            i, a, j, b = move\n            routes[i][a], routes[j][b] = routes[j][b], routes[i][a]\n            return True\n        return False\n\n    def improve_by_reinsertion(self, routes, attempts=200):\n        # attempt to reinsertion single nodes to better positions within same or other route\n        for _ in range(attempts):\n            i = self.random.randrange(len(routes))\n            if not routes[i]:\n                continue\n            pi = self.random.randrange(len(routes[i]))\n            v = routes[i].pop(pi)\n            best_gain = None\n            best = None\n            for j in range(len(routes)):\n                for pj in range(len(routes[j]) + 1):\n                    if self.route_demand(routes[j]) + self.demands[v] > self.vehicle_capacity:\n                        continue\n                    prev = 0 if pj == 0 else routes[j][pj - 1]\n                    nxt = 0 if pj == len(routes[j]) else routes[j][pj]\n                    gain = self.distance_matrix[prev, v] + self.distance_matrix[v, nxt] - self.distance_matrix[prev, nxt]\n                    if best_gain is None or gain < best_gain:\n                        best_gain = gain\n                        best = (j, pj)\n            if best is None:\n                # create new route\n                routes.append([v])\n            else:\n                j, pj = best\n                routes[j].insert(pj, v)\n        # remove empty routes\n        routes[:] = [r for r in routes if r]\n\n    # ------------------ Motif updates & adaptation ------------------\n\n    def update_motifs_post_solution(self, routes):\n        # extract motifs from improved solution and reinforce counts\n        Lmax = 4\n        for r in routes:\n            L = len(r)\n            for l in range(2, Lmax + 1):\n                for i in range(0, L - l + 1):\n                    motif = tuple(r[i:i + l])\n                    self.motif_counts[motif] += 1\n        # prune weak motifs\n        threshold = max(1, int(0.02 * sum(self.motif_counts.values())))\n        to_del = [m for m, c in self.motif_counts.items() if c < threshold]\n        for m in to_del:\n            del self.motif_counts[m]\n\n    def adapt_parameters(self, no_improve_iters):\n        # simple adaptation: increase coarsening if stagnation\n        if no_improve_iters > self.restart_trigger:\n            self.coarsen_granularity = min(0.5, self.coarsen_granularity * 1.2)\n            # diversify operators\n            self.operator_probs['two_opt'] = max(0.2, self.operator_probs['two_opt'] - 0.1)\n            self.operator_probs['relocate'] = min(0.6, self.operator_probs['relocate'] + 0.05)\n            self.operator_probs['swap'] = max(0.1, 1.0 - self.operator_probs['two_opt'] - self.operator_probs['relocate'])\n        else:\n            self.coarsen_granularity = max(0.05, self.coarsen_granularity * 0.98)\n\n    # ------------------ Perturbation / restart ------------------\n\n    def perturb_solution(self, routes, intensity=0.2):\n        # perform targeted perturbation: remove some nodes and reinsert\n        all_nodes = [(i, v) for i, r in enumerate(routes) for v in r]\n        k = max(1, int(len(all_nodes) * intensity))\n        remove = self.random.sample(all_nodes, k)\n        removed = []\n        for i, v in sorted(remove, key=lambda x: -x[0]):\n            # remove by index i\n            if v in routes[i]:\n                routes[i].remove(v)\n                removed.append(v)\n        # reinsert removed greedily\n        self.improve_by_reinsertion(routes + [[]], attempts=len(removed) * 3)\n        for v in removed:\n            best_gain = None\n            best = None\n            for j in range(len(routes)):\n                if self.route_demand(routes[j]) + self.demands[v] > self.vehicle_capacity:\n                    continue\n                for pj in range(len(routes[j]) + 1):\n                    prev = 0 if pj == 0 else routes[j][pj - 1]\n                    nxt = 0 if pj == len(routes[j]) else routes[j][pj]\n                    gain = self.distance_matrix[prev, v] + self.distance_matrix[v, nxt] - self.distance_matrix[prev, nxt]\n                    if best_gain is None or gain < best_gain:\n                        best_gain = gain\n                        best = (j, pj)\n            if best is None:\n                routes.append([v])\n            else:\n                j, pj = best\n                routes[j].insert(pj, v)\n        # cleanup empties\n        routes[:] = [r for r in routes if r]\n\n    # ------------------ Final polishing ------------------\n\n    def full_polish(self, routes, time_budget=1.0):\n        # intensive local search until time budget\n        start = time.time()\n        best_routes = [list(r) for r in routes]\n        best_cost = self.total_cost(best_routes)\n        while time.time() - start < time_budget:\n            changed = False\n            # try relocate improvements\n            for _ in range(20):\n                if self.relax_relocate(best_routes):\n                    changed = True\n            # two-opt per route\n            for i, r in enumerate(best_routes):\n                newr, ch = self.two_opt_single_route(r)\n                if ch:\n                    best_routes[i] = newr\n                    changed = True\n            # swap\n            if self.swap_between_routes(best_routes):\n                changed = True\n            if not changed:\n                break\n            new_cost = self.total_cost(best_routes)\n            if new_cost < best_cost:\n                best_cost = new_cost\n        return best_routes\n\n    # ------------------ Main solve ------------------\n\n    def solve(self) -> list:\n        \"\"\"\n        Solve the Capacitated Vehicle Routing Problem (CVRP).\n\n        Returns:\n            A one-dimensional list of integers representing the sequence of nodes visited by all vehicles.\n        \"\"\"\n        # Basic controls\n        max_iters = 300\n        time_limit = 9.0  # seconds (small budget)\n        start_time = time.time()\n\n        # Initialization: generate seeds and mine motifs\n        seeds = self.generate_seeds(num_seeds=30)\n        best_routes = min(seeds, key=lambda r: self.total_cost(r))\n        best_cost = self.total_cost(best_routes)\n        self.mine_motifs_from_seeds(seeds, top_k=80)\n        for s in seeds:\n            self.update_consensus_from_solution(s)\n\n        no_improve_iters = 0\n\n        # Main loop\n        iter_count = 0\n        while iter_count < max_iters and time.time() - start_time < time_limit:\n            iter_count += 1\n            # select a candidate: either best or a diverse seed\n            if self.random.random() < 0.7:\n                candidate = [list(r) for r in best_routes]\n            else:\n                candidate = self.generate_seed_greedy_randomized(greediness=0.3)\n\n            # Coarsening\n            coarse_data = self.build_coarse_instance(candidate, target_fraction=self.coarsen_granularity)\n\n            # Improve coarse instance\n            coarse_routes = coarse_data['routes']\n            coarse_improved = self.improve_coarse(coarse_data, iter_limit=120)\n\n            # Expand back\n            expanded = self.expand_coarse_solution(coarse_improved, coarse_data)\n\n            # Focused local refinement\n            refined = self.local_refine(expanded, iter_limit=200)\n\n            # Ensure feasibility\n            if not self.is_feasible_routes(refined):\n                refined = self.full_polish(refined, time_budget=0.1)\n\n            refined_cost = self.total_cost(refined)\n\n            # Update consensus and motifs\n            self.update_consensus_from_solution(refined)\n            self.update_motifs_post_solution(refined)\n\n            # Adapt parameters\n            if refined_cost < best_cost - 1e-6:\n                best_routes = refined\n                best_cost = refined_cost\n                no_improve_iters = 0\n            else:\n                no_improve_iters += 1\n            self.adapt_parameters(no_improve_iters)\n\n            # occasional perturbation or restart\n            if no_improve_iters > self.max_no_improve:\n                # build a new solution from motifs\n                motif_based = []\n                # try assemble routes using top motifs greedily\n                remaining = set(range(1, self.n))\n                motifs = [m for m, _ in self.motif_counts.most_common(40)]\n                while remaining:\n                    route = []\n                    cap = 0\n                    # try to place motifs\n                    placed_any = False\n                    for m in motifs:\n                        if set(m).issubset(remaining) and cap + sum(self.demands[v] for v in m) <= self.vehicle_capacity:\n                            route.extend(m)\n                            for v in m:\n                                remaining.remove(v)\n                            cap += sum(self.demands[v] for v in m)\n                            placed_any = True\n                    # fill with nearest\n                    cur = 0\n                    while True:\n                        candidates = [v for v in remaining if cap + self.demands[v] <= self.vehicle_capacity]\n                        if not candidates:\n                            break\n                        # nearest\n                        candidates.sort(key=lambda v: self.distance_matrix[cur, v])\n                        v = candidates[0]\n                        route.append(v)\n                        remaining.remove(v)\n                        cap += self.demands[v]\n                        cur = v\n                    motif_based.append(route)\n                # local refine motif_based\n                motif_based = self.local_refine(motif_based, iter_limit=200)\n                if self.total_cost(motif_based) < best_cost:\n                    best_routes = motif_based\n                    best_cost = self.total_cost(motif_based)\n                no_improve_iters = 0\n                # broaden exploration\n                self.coarsen_granularity = max(0.1, self.coarsen_granularity * 0.7)\n            elif no_improve_iters > self.restart_trigger and self.random.random() < 0.5:\n                # perturb current best\n                self.perturb_solution(best_routes, intensity=0.15)\n\n        # Post-processing polishing\n        polished = self.full_polish(best_routes, time_budget=1.0)\n        if self.total_cost(polished) < best_cost:\n            best_routes = polished\n            best_cost = self.total_cost(polished)\n\n        # produce final flat solution\n        final_solution = self.solution_from_routes(best_routes)\n        # Ensure feasibility: if somehow not feasible, fallback to greedy simple packing\n        if not self.is_feasible_routes(best_routes):\n            routes = self.generate_seed_greedy_randomized(greediness=0.0)\n            final_solution = self.solution_from_routes(routes)\n        return final_solution",
            "# Your program here\nimport numpy as np\nimport random\nimport time\nfrom collections import defaultdict, Counter\nfrom copy import deepcopy\n\nclass FSSPSolver:\n    def __init__(self, num_jobs: int, num_machines: int, processing_times: list):\n        \"\"\"\n        Initialize the FSSP solver.\n\n        Args:\n            num_jobs: Number of jobs in the problem\n            num_machines: Number of machines in the problem\n            processing_times: List of lists where processing_times[j][m] is the processing time of job j on machine m\n        \"\"\"\n        self.num_jobs = num_jobs\n        self.num_machines = num_machines\n        self.pt = processing_times\n        # Precompute job total processing times (useful for heuristics)\n        self.job_totals = [sum(self.pt[j]) for j in range(self.num_jobs)]\n        # Seed RNG\n        random.seed(42)\n\n    def _makespan(self, seq):\n        \"\"\"Compute flow shop makespan for a given job sequence.\"\"\"\n        n = len(seq)\n        if n == 0:\n            return 0\n        m = self.num_machines\n        # Use two arrays to compute cumulative times\n        prev = [0] * (m + 1)\n        for job in seq:\n            cur = [0] * (m + 1)\n            for machine in range(1, m + 1):\n                cur[machine] = max(prev[machine], cur[machine - 1]) + self.pt[job][machine - 1]\n            prev = cur\n        return prev[m]\n\n    def _neh(self):\n        \"\"\"NEH heuristic for initial seed solution.\"\"\"\n        jobs_sorted = sorted(range(self.num_jobs), key=lambda j: self.job_totals[j], reverse=True)\n        seq = []\n        for j in jobs_sorted:\n            best_seq = None\n            best_val = None\n            # try inserting j in all positions\n            for pos in range(len(seq) + 1):\n                cand = seq[:pos] + [j] + seq[pos:]\n                val = self._makespan(cand)\n                if best_val is None or val < best_val:\n                    best_val = val\n                    best_seq = cand\n            seq = best_seq\n        return seq\n\n    def _random_greedy(self):\n        \"\"\"Randomized greedy insertion based on job totals + noise.\"\"\"\n        jobs = list(range(self.num_jobs))\n        random.shuffle(jobs)\n        seq = []\n        for j in jobs:\n            best_seq = None\n            best_val = None\n            for pos in range(len(seq) + 1):\n                cand = seq[:pos] + [j] + seq[pos:]\n                val = self._makespan(cand)\n                # add small noise to encourage diversity\n                val_noise = val + random.uniform(0, 1e-6)\n                if best_val is None or val_noise < best_val:\n                    best_val = val_noise\n                    best_seq = cand\n            seq = best_seq\n        return seq\n\n    def _local_search_improve(self, seq, time_budget_iters=200):\n        \"\"\"Best-improvement insertion local search (fine-grained).\"\"\"\n        best = list(seq)\n        best_val = self._makespan(best)\n        n = len(best)\n        iters = 0\n        improved = True\n        while improved and iters < time_budget_iters:\n            improved = False\n            iters += 1\n            # try all insertions (including swaps realized as insertion)\n            for i in range(n):\n                for j in range(n):\n                    if i == j:\n                        continue\n                    cand = best[:]\n                    job = cand.pop(i)\n                    cand.insert(j, job)\n                    val = self._makespan(cand)\n                    if val < best_val:\n                        best = cand\n                        best_val = val\n                        improved = True\n                        break\n                if improved:\n                    break\n        return best, best_val\n\n    def _two_opt_on_sequence(self, seq, iter_limit=100):\n        \"\"\"2-opt like swap of blocks (coarse).\"\"\"\n        best = list(seq)\n        best_val = self._makespan(best)\n        n = len(best)\n        it = 0\n        while it < iter_limit:\n            improved = False\n            it += 1\n            for i in range(n - 1):\n                for j in range(i + 1, n):\n                    cand = best[:i] + best[i:j+1][::-1] + best[j+1:]\n                    val = self._makespan(cand)\n                    if val < best_val:\n                        best = cand\n                        best_val = val\n                        improved = True\n                        break\n                if improved:\n                    break\n            if not improved:\n                break\n        return best, best_val\n\n    def _mine_motifs(self, seeds, max_len=4, min_freq=2):\n        \"\"\"Mine frequent contiguous subsequences (motifs) from seed solutions.\"\"\"\n        counts = Counter()\n        score_sum = defaultdict(float)\n        for seq, val in seeds:\n            n = len(seq)\n            for L in range(2, min(max_len, n) + 1):\n                for i in range(n - L + 1):\n                    motif = tuple(seq[i:i+L])\n                    counts[motif] += 1\n                    score_sum[motif] += val\n        motifs = {}\n        for motif, cnt in counts.items():\n            if cnt >= min_freq:\n                avg_score = score_sum[motif] / cnt\n                # store motif with support and average score (lower is better)\n                motifs[motif] = (cnt, avg_score)\n        # sort motifs by support and length and score\n        sorted_motifs = sorted(motifs.items(), key=lambda kv: (-kv[1][0], len(kv[0]), kv[1][1]))\n        return [m for m, _ in sorted_motifs]\n\n    def _build_consensus(self, seeds):\n        \"\"\"Build consensus positions (average positions for each job).\"\"\"\n        pos_acc = defaultdict(list)\n        for seq, _ in seeds:\n            for i, job in enumerate(seq):\n                pos_acc[job].append(i)\n        consensus = {}\n        for job in range(self.num_jobs):\n            if job in pos_acc:\n                consensus[job] = sum(pos_acc[job]) / len(pos_acc[job])\n            else:\n                consensus[job] = self.num_jobs / 2.0\n        return consensus\n\n    def _consensus_sequence(self, consensus, perturb=0.2):\n        \"\"\"Create a sequence from consensus by sorting jobs by average position, with small perturbations.\"\"\"\n        jobs = list(range(self.num_jobs))\n        # add small noise based on perturb\n        jobs.sort(key=lambda j: consensus.get(j, self.num_jobs/2.0) + random.uniform(-perturb, perturb))\n        return jobs\n\n    def _aggregate_motifs_into_macros(self, seq, motifs):\n        \"\"\"\n        Aggregate non-overlapping motifs found in seq into macro-entities.\n        motifs: list of tuples (motif sequences) prioritized externally.\n        \"\"\"\n        n = len(seq)\n        used = [False] * n\n        macro_list = []\n        pos_to_idx = {i: i for i in range(n)}\n        # map job positions to their index in seq\n        i = 0\n        while i < n:\n            placed = False\n            # try longest motifs first\n            for motif in motifs:\n                L = len(motif)\n                if i + L <= n and not any(used[i:i+L]) and tuple(seq[i:i+L]) == tuple(motif):\n                    macro_list.append(list(motif))\n                    for k in range(i, i+L):\n                        used[k] = True\n                    i += L\n                    placed = True\n                    break\n            if not placed:\n                # single job macro\n                if not used[i]:\n                    macro_list.append([seq[i]])\n                    used[i] = True\n                i += 1\n        return macro_list\n\n    def _expand_macros(self, macro_seq):\n        \"\"\"Flatten macro-sequence to job sequence.\"\"\"\n        seq = []\n        seen = set()\n        for macro in macro_seq:\n            for job in macro:\n                if job not in seen:\n                    seq.append(job)\n                    seen.add(job)\n        # append any missing jobs (shouldn't happen usually)\n        if len(seq) < self.num_jobs:\n            for j in range(self.num_jobs):\n                if j not in seen:\n                    seq.append(j)\n        return seq\n\n    def _repair_sequence(self, seq):\n        \"\"\"Ensure seq is a permutation of jobs: remove duplicates, append missing.\"\"\"\n        seen = set()\n        repaired = []\n        for j in seq:\n            if j not in seen and 0 <= j < self.num_jobs:\n                repaired.append(j)\n                seen.add(j)\n        for j in range(self.num_jobs):\n            if j not in seen:\n                repaired.append(j)\n                seen.add(j)\n        return repaired\n\n    def solve(self) -> list:\n        \"\"\"\n        Solve the Flow Shop Scheduling Problem (FSSP).\n\n        Returns:\n            A list representing the sequence of jobs to be processed.\n        \"\"\"\n        # Parameters and adaptive controls\n        max_iters = max(100, 10 * self.num_jobs)\n        time_limit = 5.0  # seconds budget for algorithm (reasonable default)\n        start_time = time.time()\n\n        # --- Initialization Phase ---\n        seeds = []\n        # NEH seed\n        neh_seq = self._neh()\n        seeds.append((neh_seq, self._makespan(neh_seq)))\n        # Random greedy seeds\n        for _ in range(min(6, max(1, self.num_jobs // 2))):\n            rg = self._random_greedy()\n            seeds.append((rg, self._makespan(rg)))\n        # Random seeds\n        for _ in range(min(6, self.num_jobs)):\n            r = list(range(self.num_jobs))\n            random.shuffle(r)\n            seeds.append((r, self._makespan(r)))\n        # Keep unique seeds and sort\n        seen_str = set()\n        uniq_seeds = []\n        for seq, val in seeds:\n            key = tuple(seq)\n            if key not in seen_str:\n                seen_str.add(key)\n                uniq_seeds.append((seq, val))\n        # Sort seeds by value\n        uniq_seeds.sort(key=lambda x: x[1])\n        seeds = uniq_seeds[:min(20, len(uniq_seeds))]\n\n        # Build initial motif library and consensus memory\n        motifs = self._mine_motifs(seeds, max_len=min(5, self.num_jobs), min_freq=2)\n        consensus = self._build_consensus(seeds)\n\n        # Control parameters\n        no_improve = 0\n        best_seq, best_val = seeds[0]\n        best_seq = list(best_seq)\n        best_val = float(best_val)\n\n        # Operator probabilities (coarse/fine)\n        coarse_prob = 0.5\n        motif_use_prob = 0.7\n        perturb_prob = 0.15\n\n        iter_count = 0\n        # Main Processing Loop\n        while iter_count < max_iters and (time.time() - start_time) < time_limit:\n            iter_count += 1\n\n            # Select candidates guided by quality, feasibility margin, novelty\n            # pick top seed and one more by novelty (max distance from consensus)\n            seeds.sort(key=lambda s: s[1])\n            parent1 = deepcopy(seeds[0][0])\n            # novelty: sequence distance to consensus ordering\n            consensus_seq = self._consensus_sequence(consensus, perturb=0.1)\n            pos_cons = {job: i for i, job in enumerate(consensus_seq)}\n            def novelty_score(seq):\n                return sum(abs(i - pos_cons[seq[i]]) for i in range(len(seq)))\n            candidate = max(seeds, key=lambda s: novelty_score(s[0]))[0]\n            parent2 = deepcopy(candidate) if random.random() < 0.8 else deepcopy(seeds[min(1, len(seeds)-1)][0])\n\n            # Construct multilevel representation by aggregating motifs and consensus patterns\n            select_motifs = motifs[:min(len(motifs), max(1, int(len(motifs) * motif_use_prob)))]\n            # Build macro sequence from parent1 trying to respect motifs\n            macro_seq = self._aggregate_motifs_into_macros(parent1, select_motifs)\n            # If coarse search required, apply macro-level operators\n            if random.random() < coarse_prob and len(macro_seq) > 1:\n                # Permute macros by insertion / 2-opt at macro level\n                # Represent macro_seq as list of macros; evaluate expanded makespan\n                def eval_macro_order(macros):\n                    expanded = self._expand_macros(macros)\n                    return self._makespan(expanded)\n                # Start from macro_seq\n                cur_macros = deepcopy(macro_seq)\n                cur_val = eval_macro_order(cur_macros)\n                # Try macro-level 2-opt and random insertions\n                improved = True\n                macro_iters = 0\n                while improved and macro_iters < 20:\n                    improved = False\n                    macro_iters += 1\n                    # 2-opt on macro list\n                    L = len(cur_macros)\n                    for i in range(L - 1):\n                        for j in range(i + 1, L):\n                            cand = cur_macros[:i] + cur_macros[i:j+1][::-1] + cur_macros[j+1:]\n                            val = eval_macro_order(cand)\n                            if val < cur_val:\n                                cur_macros = cand\n                                cur_val = val\n                                improved = True\n                                break\n                        if improved:\n                            break\n                    if not improved:\n                        # try random macro insertion moves\n                        for _ in range(10):\n                            a = random.randrange(len(cur_macros))\n                            b = random.randrange(len(cur_macros))\n                            if a == b:\n                                continue\n                            tmp = cur_macros[:]\n                            item = tmp.pop(a)\n                            tmp.insert(b, item)\n                            val = eval_macro_order(tmp)\n                            if val < cur_val:\n                                cur_macros = tmp\n                                cur_val = val\n                                improved = True\n                                break\n                macro_seq = cur_macros\n\n            # Mix coarse recombination with fine-grained moves on expanded sequence\n            expanded_seq = self._expand_macros(macro_seq)\n            expanded_seq = self._repair_sequence(expanded_seq)\n\n            # Light feasibility repair (permutation already ensured), but ensure all jobs present\n            expanded_seq = self._repair_sequence(expanded_seq)\n\n            # Focused local refinement targeting boundaries: do short local search around motif boundaries\n            # Identify motif boundaries positions\n            boundaries = set()\n            idx = 0\n            for macro in macro_seq:\n                idx += len(macro)\n                boundaries.add(max(0, idx-1))\n            # perform local insertions focused around boundaries\n            refined = list(expanded_seq)\n            best_local_val = self._makespan(refined)\n            # perform a limited number of targeted insertions near boundaries\n            n = len(refined)\n            attempts = 0\n            while attempts < 100:\n                attempts += 1\n                i = random.randrange(n)\n                j = random.randrange(n)\n                # prefer moves if i or j near a boundary\n                if not any(abs(i - b) <= 2 or abs(j - b) <= 2 for b in boundaries):\n                    if random.random() < 0.7:\n                        continue\n                cand = refined[:]\n                job = cand.pop(i)\n                cand.insert(j, job)\n                val = self._makespan(cand)\n                if val < best_local_val:\n                    refined = cand\n                    best_local_val = val\n            # Also run a short full local search\n            refined, best_local_val = self._local_search_improve(refined, time_budget_iters=100)\n\n            # Update best solution\n            if best_local_val < best_val:\n                best_seq = list(refined)\n                best_val = best_local_val\n                no_improve = 0\n            else:\n                no_improve += 1\n\n            # Update motif library and consensus memory by extracting robust patterns from improved solution\n            # Add refined to seeds pool\n            seeds.append((list(refined), best_local_val))\n            # keep seeds top-K unique\n            uniq = {}\n            for seq, val in seeds:\n                key = tuple(seq)\n                if key not in uniq or val < uniq[key]:\n                    uniq[key] = val\n            seeds = sorted(list(uniq.items()), key=lambda kv: kv[1])\n            seeds = [(list(k), v) for k, v in seeds[:50]]\n\n            # Dynamically update motifs every few iterations\n            if iter_count % 5 == 0:\n                new_motifs = self._mine_motifs(seeds, max_len=min(6, self.num_jobs), min_freq=2)\n                # merge with previous: keep intersection or strong ones\n                combined = []\n                seen_m = set()\n                for m in new_motifs + motifs:\n                    if m not in seen_m:\n                        combined.append(m)\n                        seen_m.add(m)\n                motifs = combined[:min(100, len(combined))]\n\n            # Adapt control parameters\n            if no_improve > 10:\n                coarse_prob = min(0.9, coarse_prob + 0.05)\n                motif_use_prob = max(0.2, motif_use_prob - 0.05)\n            else:\n                coarse_prob = max(0.3, coarse_prob - 0.01)\n                motif_use_prob = min(0.9, motif_use_prob + 0.01)\n\n            # Occasionally perform targeted perturbation/restart seeded from consensus\n            if no_improve > 20 or random.random() < perturb_prob:\n                # build consensus and perturb it\n                consensus = self._build_consensus(seeds)\n                seed_from_cons = self._consensus_sequence(consensus, perturb=0.5)\n                # apply random block shuffles\n                s = seed_from_cons[:]\n                for _ in range(max(1, self.num_jobs // 5)):\n                    a = random.randrange(self.num_jobs)\n                    b = random.randrange(self.num_jobs)\n                    if a > b:\n                        a, b = b, a\n                    s = s[:a] + s[a:b+1][::-1] + s[b+1:]\n                s = self._repair_sequence(s)\n                s, val = self._local_search_improve(s, time_budget_iters=100)\n                seeds.append((s, val))\n                no_improve = 0  # reset stagnation\n\n            # update consensus every iteration\n            consensus = self._build_consensus(seeds)\n\n        # Post-Processing Phase: final polishing on best_seq\n        polished_seq = list(best_seq)\n        # run more intensive local search (iterated greedy style)\n        polished_val = best_val\n        for _ in range(50):\n            # random destruction: remove k jobs and reinsert greedily\n            k = max(1, int(0.08 * self.num_jobs))\n            remove_idx = sorted(random.sample(range(self.num_jobs), k), reverse=True)\n            partial = polished_seq[:]\n            removed = []\n            for idx in remove_idx:\n                removed.append(partial.pop(idx))\n            # reinsert removed greedily\n            for job in removed:\n                best_pos = None\n                best_val_local = None\n                for pos in range(len(partial) + 1):\n                    cand = partial[:pos] + [job] + partial[pos:]\n                    val = self._makespan(cand)\n                    if best_val_local is None or val < best_val_local:\n                        best_val_local = val\n                        best_pos = pos\n                partial = partial[:best_pos] + [job] + partial[best_pos:]\n            partial, val = self._local_search_improve(partial, time_budget_iters=200)\n            if val < polished_val:\n                polished_seq = partial\n                polished_val = val\n\n        # Final repair and return\n        polished_seq = self._repair_sequence(polished_seq)\n        return polished_seq"
        ],
        "function_bodies": [
            "# Your key function here\ndef two_opt(self, tour: np.ndarray, max_passes: int = 50, candidate_edges: Set[Tuple[int,int]] = None) -> np.ndarray:\n    \"\"\"\n    Optimized 2-opt with don't-look bits and quick positional updates.\n    Keeps the same signature as original but reduces redundant checks and\n    minimizes Python overhead for large instances.\n    \"\"\"\n    n = len(tour)\n    if n < 4:\n        return tour.copy()\n\n    dist = self.distance_matrix\n    tour = tour.copy()\n    # position of each node in the tour for O(1) index updates if needed\n    pos = np.empty(n, dtype=int)\n    pos[tour] = np.arange(n)\n\n    # don't-look bits: if a node has been scanned without improvement, we skip it\n    dlb = np.ones(n, dtype=bool)\n\n    passes = 0\n    improved_any = True\n    rng = random.random\n\n    # local references to speed up attribute access in loops\n    eps = 1e-12\n\n    while improved_any and passes < max_passes:\n        passes += 1\n        improved_any = False\n        # iterate over indices; using range over positions, not node labels\n        for i in range(n):\n            if not dlb[i]:\n                continue\n            a = tour[i]\n            b = tour[(i + 1) % n]\n            dlb[i] = False  # we've examined edge (a,b)\n            # scan for a beneficial 2-opt with edge (a,b)\n            improved = False\n            # j ranges from i+2 .. n-1, but skip (i==0 and j==n-1)\n            j = i + 2\n            while j < n:\n                if i == 0 and j == n - 1:\n                    j += 1\n                    continue\n                c = tour[j]\n                d = tour[(j + 1) % n]\n                # candidate pruning if provided: require at least one involved edge to be in candidate set\n                if candidate_edges is not None:\n                    e1 = (min(a, b), max(a, b))\n                    e2 = (min(c, d), max(c, d))\n                    if e1 not in candidate_edges and e2 not in candidate_edges:\n                        # allow occasional non-candidate check to avoid getting stuck\n                        if rng() > 0.05:\n                            j += 1\n                            continue\n                # compute 2-opt gain\n                delta = dist[a, c] + dist[b, d] - dist[a, b] - dist[c, d]\n                if delta < -eps:\n                    # perform reversal of segment (i+1 .. j) inclusive\n                    # use numpy slice reversal which is efficient in C\n                    tour[i+1:j+1] = tour[i+1:j+1][::-1]\n                    # update positions for affected nodes\n                    seg = tour[i+1:j+1]\n                    pos[seg] = np.arange(i+1, j+1)\n                    improved = True\n                    improved_any = True\n                    # mark nearby positions as worth checking again\n                    dlb[i] = True\n                    dlb[(i + 1) % n] = True\n                    dlb[j] = True\n                    dlb[(j + 1) % n] = True\n                    break  # restart scanning from beginning of outer for-loop\n                j += 1\n            if improved:\n                # break to restart scanning with don't-look bits set appropriately\n                break\n        # if no improvement in this pass, we'll exit loop\n    return tour",
            "# Your key function here\ndef two_opt_single_route(self, route):\n    \"\"\"\n    Optimized 2-opt for a single route using O(n^2) delta computations\n    instead of recomputing full route costs for every candidate.\n    \"\"\"\n    n = len(route)\n    if n < 4:\n        return route, False\n\n    dist = self.distance_matrix  # local ref for speed\n    best_delta = 0.0\n    best_i = -1\n    best_j = -1\n\n    # For a reversal between i..j, the cost change (delta) is:\n    # -dist(prev, r[i]) - dist(r[j], next) + dist(prev, r[j]) + dist(r[i], next)\n    # where prev = 0 if i==0 else r[i-1], next = 0 if j==n-1 else r[j+1]\n    for i in range(n - 1):\n        prev = 0 if i == 0 else route[i - 1]\n        ri = route[i]\n        # j starts from i+1 (valid 2-opt segment)\n        for j in range(i + 1, n):\n            rj = route[j]\n            nxt = 0 if j == n - 1 else route[j + 1]\n            delta = -dist[prev, ri] - dist[rj, nxt] + dist[prev, rj] + dist[ri, nxt]\n            if delta < best_delta:\n                best_delta = delta\n                best_i = i\n                best_j = j\n\n    if best_i >= 0:\n        new_route = route[:best_i] + list(reversed(route[best_i:best_j + 1])) + route[best_j + 1:]\n        return new_route, True\n\n    return route, False",
            "# Your key function here\ndef _makespan(self, seq):\n    \"\"\"Compute flow shop makespan for a given job sequence (optimized in-place).\"\"\"\n    if not seq:\n        return 0\n    m = self.num_machines\n    # fast local references\n    pt = self.pt\n    # prev[k] will hold the completion time on machine k after processing processed jobs so far\n    prev = [0] * m\n\n    # Iterate jobs and update prev in-place (no per-job allocation of a new array)\n    for job in seq:\n        job_pt = pt[job]\n        # machine 0: straightforward\n        cur = prev[0] + job_pt[0]\n        prev[0] = cur\n        # machines 1..m-1: use cur as \"current job completion on previous machine\"\n        for k in range(1, m):\n            # completion is max(prev[k] (previous job on this machine), cur (current job on previous machine)) + processing\n            other = prev[k]\n            if other > cur:\n                cur = other + job_pt[k]\n            else:\n                cur = cur + job_pt[k]\n            prev[k] = cur\n    return prev[-1]"
        ]
    },
'''

import numpy as np
import random
import time
from collections import defaultdict, Counter
from copy import deepcopy

class FSSPSolver:
    def __init__(self, num_jobs: int, num_machines: int, processing_times: list):
        """
        Initialize the FSSP solver.

        Args:
            num_jobs: Number of jobs in the problem
            num_machines: Number of machines in the problem
            processing_times: List of lists where processing_times[j][m] is the processing time of job j on machine m
        """
        self.num_jobs = num_jobs
        self.num_machines = num_machines
        self.pt = processing_times
        # Precompute job total processing times (useful for heuristics)
        self.job_totals = [sum(self.pt[j]) for j in range(self.num_jobs)]
        # Seed RNG
        random.seed(42)

    def _makespan(self, seq):
        """Compute flow shop makespan for a given job sequence."""
        n = len(seq)
        if n == 0:
            return 0
        m = self.num_machines
        # Use two arrays to compute cumulative times
        prev = [0] * (m + 1)
        for job in seq:
            cur = [0] * (m + 1)
            for machine in range(1, m + 1):
                cur[machine] = max(prev[machine], cur[machine - 1]) + self.pt[job][machine - 1]
            prev = cur
        return prev[m]

    def _neh(self):
        """NEH heuristic for initial seed solution."""
        jobs_sorted = sorted(range(self.num_jobs), key=lambda j: self.job_totals[j], reverse=True)
        seq = []
        for j in jobs_sorted:
            best_seq = None
            best_val = None
            # try inserting j in all positions
            for pos in range(len(seq) + 1):
                cand = seq[:pos] + [j] + seq[pos:]
                val = self._makespan(cand)
                if best_val is None or val < best_val:
                    best_val = val
                    best_seq = cand
            seq = best_seq
        return seq

    def _random_greedy(self):
        """Randomized greedy insertion based on job totals + noise."""
        jobs = list(range(self.num_jobs))
        random.shuffle(jobs)
        seq = []
        for j in jobs:
            best_seq = None
            best_val = None
            for pos in range(len(seq) + 1):
                cand = seq[:pos] + [j] + seq[pos:]
                val = self._makespan(cand)
                # add small noise to encourage diversity
                val_noise = val + random.uniform(0, 1e-6)
                if best_val is None or val_noise < best_val:
                    best_val = val_noise
                    best_seq = cand
            seq = best_seq
        return seq

    def _local_search_improve(self, seq, time_budget_iters=200):
        """Best-improvement insertion local search (fine-grained)."""
        best = list(seq)
        best_val = self._makespan(best)
        n = len(best)
        iters = 0
        improved = True
        while improved and iters < time_budget_iters:
            improved = False
            iters += 1
            # try all insertions (including swaps realized as insertion)
            for i in range(n):
                for j in range(n):
                    if i == j:
                        continue
                    cand = best[:]
                    job = cand.pop(i)
                    cand.insert(j, job)
                    val = self._makespan(cand)
                    if val < best_val:
                        best = cand
                        best_val = val
                        improved = True
                        break
                if improved:
                    break
        return best, best_val

    def _two_opt_on_sequence(self, seq, iter_limit=100):
        """2-opt like swap of blocks (coarse)."""
        best = list(seq)
        best_val = self._makespan(best)
        n = len(best)
        it = 0
        while it < iter_limit:
            improved = False
            it += 1
            for i in range(n - 1):
                for j in range(i + 1, n):
                    cand = best[:i] + best[i:j+1][::-1] + best[j+1:]
                    val = self._makespan(cand)
                    if val < best_val:
                        best = cand
                        best_val = val
                        improved = True
                        break
                if improved:
                    break
            if not improved:
                break
        return best, best_val

    def _mine_motifs(self, seeds, max_len=4, min_freq=2):
        """Mine frequent contiguous subsequences (motifs) from seed solutions."""
        counts = Counter()
        score_sum = defaultdict(float)
        for seq, val in seeds:
            n = len(seq)
            for L in range(2, min(max_len, n) + 1):
                for i in range(n - L + 1):
                    motif = tuple(seq[i:i+L])
                    counts[motif] += 1
                    score_sum[motif] += val
        motifs = {}
        for motif, cnt in counts.items():
            if cnt >= min_freq:
                avg_score = score_sum[motif] / cnt
                # store motif with support and average score (lower is better)
                motifs[motif] = (cnt, avg_score)
        # sort motifs by support and length and score
        sorted_motifs = sorted(motifs.items(), key=lambda kv: (-kv[1][0], len(kv[0]), kv[1][1]))
        return [m for m, _ in sorted_motifs]

    def _build_consensus(self, seeds):
        """Build consensus positions (average positions for each job)."""
        pos_acc = defaultdict(list)
        for seq, _ in seeds:
            for i, job in enumerate(seq):
                pos_acc[job].append(i)
        consensus = {}
        for job in range(self.num_jobs):
            if job in pos_acc:
                consensus[job] = sum(pos_acc[job]) / len(pos_acc[job])
            else:
                consensus[job] = self.num_jobs / 2.0
        return consensus

    def _consensus_sequence(self, consensus, perturb=0.2):
        """Create a sequence from consensus by sorting jobs by average position, with small perturbations."""
        jobs = list(range(self.num_jobs))
        # add small noise based on perturb
        jobs.sort(key=lambda j: consensus.get(j, self.num_jobs/2.0) + random.uniform(-perturb, perturb))
        return jobs

    def _aggregate_motifs_into_macros(self, seq, motifs):
        """
        Aggregate non-overlapping motifs found in seq into macro-entities.
        motifs: list of tuples (motif sequences) prioritized externally.
        """
        n = len(seq)
        used = [False] * n
        macro_list = []
        pos_to_idx = {i: i for i in range(n)}
        # map job positions to their index in seq
        i = 0
        while i < n:
            placed = False
            # try longest motifs first
            for motif in motifs:
                L = len(motif)
                if i + L <= n and not any(used[i:i+L]) and tuple(seq[i:i+L]) == tuple(motif):
                    macro_list.append(list(motif))
                    for k in range(i, i+L):
                        used[k] = True
                    i += L
                    placed = True
                    break
            if not placed:
                # single job macro
                if not used[i]:
                    macro_list.append([seq[i]])
                    used[i] = True
                i += 1
        return macro_list

    def _expand_macros(self, macro_seq):
        """Flatten macro-sequence to job sequence."""
        seq = []
        seen = set()
        for macro in macro_seq:
            for job in macro:
                if job not in seen:
                    seq.append(job)
                    seen.add(job)
        # append any missing jobs (shouldn't happen usually)
        if len(seq) < self.num_jobs:
            for j in range(self.num_jobs):
                if j not in seen:
                    seq.append(j)
        return seq

    def _repair_sequence(self, seq):
        """Ensure seq is a permutation of jobs: remove duplicates, append missing."""
        seen = set()
        repaired = []
        for j in seq:
            if j not in seen and 0 <= j < self.num_jobs:
                repaired.append(j)
                seen.add(j)
        for j in range(self.num_jobs):
            if j not in seen:
                repaired.append(j)
                seen.add(j)
        return repaired

    def solve(self) -> list:
        """
        Solve the Flow Shop Scheduling Problem (FSSP).

        Returns:
            A list representing the sequence of jobs to be processed.
        """
        # Parameters and adaptive controls
        max_iters = max(100, 10 * self.num_jobs)
        time_limit = 5.0  # seconds budget for algorithm (reasonable default)
        start_time = time.time()

        # --- Initialization Phase ---
        seeds = []
        # NEH seed
        neh_seq = self._neh()
        seeds.append((neh_seq, self._makespan(neh_seq)))
        # Random greedy seeds
        for _ in range(min(6, max(1, self.num_jobs // 2))):
            rg = self._random_greedy()
            seeds.append((rg, self._makespan(rg)))
        # Random seeds
        for _ in range(min(6, self.num_jobs)):
            r = list(range(self.num_jobs))
            random.shuffle(r)
            seeds.append((r, self._makespan(r)))
        # Keep unique seeds and sort
        seen_str = set()
        uniq_seeds = []
        for seq, val in seeds:
            key = tuple(seq)
            if key not in seen_str:
                seen_str.add(key)
                uniq_seeds.append((seq, val))
        # Sort seeds by value
        uniq_seeds.sort(key=lambda x: x[1])
        seeds = uniq_seeds[:min(20, len(uniq_seeds))]

        # Build initial motif library and consensus memory
        motifs = self._mine_motifs(seeds, max_len=min(5, self.num_jobs), min_freq=2)
        consensus = self._build_consensus(seeds)

        # Control parameters
        no_improve = 0
        best_seq, best_val = seeds[0]
        best_seq = list(best_seq)
        best_val = float(best_val)

        # Operator probabilities (coarse/fine)
        coarse_prob = 0.5
        motif_use_prob = 0.7
        perturb_prob = 0.15

        iter_count = 0
        # Main Processing Loop
        while iter_count < max_iters and (time.time() - start_time) < time_limit:
            iter_count += 1

            # Select candidates guided by quality, feasibility margin, novelty
            # pick top seed and one more by novelty (max distance from consensus)
            seeds.sort(key=lambda s: s[1])
            parent1 = deepcopy(seeds[0][0])
            # novelty: sequence distance to consensus ordering
            consensus_seq = self._consensus_sequence(consensus, perturb=0.1)
            pos_cons = {job: i for i, job in enumerate(consensus_seq)}
            def novelty_score(seq):
                return sum(abs(i - pos_cons[seq[i]]) for i in range(len(seq)))
            candidate = max(seeds, key=lambda s: novelty_score(s[0]))[0]
            parent2 = deepcopy(candidate) if random.random() < 0.8 else deepcopy(seeds[min(1, len(seeds)-1)][0])

            # Construct multilevel representation by aggregating motifs and consensus patterns
            select_motifs = motifs[:min(len(motifs), max(1, int(len(motifs) * motif_use_prob)))]
            # Build macro sequence from parent1 trying to respect motifs
            macro_seq = self._aggregate_motifs_into_macros(parent1, select_motifs)
            # If coarse search required, apply macro-level operators
            if random.random() < coarse_prob and len(macro_seq) > 1:
                # Permute macros by insertion / 2-opt at macro level
                # Represent macro_seq as list of macros; evaluate expanded makespan
                def eval_macro_order(macros):
                    expanded = self._expand_macros(macros)
                    return self._makespan(expanded)
                # Start from macro_seq
                cur_macros = deepcopy(macro_seq)
                cur_val = eval_macro_order(cur_macros)
                # Try macro-level 2-opt and random insertions
                improved = True
                macro_iters = 0
                while improved and macro_iters < 20:
                    improved = False
                    macro_iters += 1
                    # 2-opt on macro list
                    L = len(cur_macros)
                    for i in range(L - 1):
                        for j in range(i + 1, L):
                            cand = cur_macros[:i] + cur_macros[i:j+1][::-1] + cur_macros[j+1:]
                            val = eval_macro_order(cand)
                            if val < cur_val:
                                cur_macros = cand
                                cur_val = val
                                improved = True
                                break
                        if improved:
                            break
                    if not improved:
                        # try random macro insertion moves
                        for _ in range(10):
                            a = random.randrange(len(cur_macros))
                            b = random.randrange(len(cur_macros))
                            if a == b:
                                continue
                            tmp = cur_macros[:]
                            item = tmp.pop(a)
                            tmp.insert(b, item)
                            val = eval_macro_order(tmp)
                            if val < cur_val:
                                cur_macros = tmp
                                cur_val = val
                                improved = True
                                break
                macro_seq = cur_macros

            # Mix coarse recombination with fine-grained moves on expanded sequence
            expanded_seq = self._expand_macros(macro_seq)
            expanded_seq = self._repair_sequence(expanded_seq)

            # Light feasibility repair (permutation already ensured), but ensure all jobs present
            expanded_seq = self._repair_sequence(expanded_seq)

            # Focused local refinement targeting boundaries: do short local search around motif boundaries
            # Identify motif boundaries positions
            boundaries = set()
            idx = 0
            for macro in macro_seq:
                idx += len(macro)
                boundaries.add(max(0, idx-1))
            # perform local insertions focused around boundaries
            refined = list(expanded_seq)
            best_local_val = self._makespan(refined)
            # perform a limited number of targeted insertions near boundaries
            n = len(refined)
            attempts = 0
            while attempts < 100:
                attempts += 1
                i = random.randrange(n)
                j = random.randrange(n)
                # prefer moves if i or j near a boundary
                if not any(abs(i - b) <= 2 or abs(j - b) <= 2 for b in boundaries):
                    if random.random() < 0.7:
                        continue
                cand = refined[:]
                job = cand.pop(i)
                cand.insert(j, job)
                val = self._makespan(cand)
                if val < best_local_val:
                    refined = cand
                    best_local_val = val
            # Also run a short full local search
            refined, best_local_val = self._local_search_improve(refined, time_budget_iters=100)

            # Update best solution
            if best_local_val < best_val:
                best_seq = list(refined)
                best_val = best_local_val
                no_improve = 0
            else:
                no_improve += 1

            # Update motif library and consensus memory by extracting robust patterns from improved solution
            # Add refined to seeds pool
            seeds.append((list(refined), best_local_val))
            # keep seeds top-K unique
            uniq = {}
            for seq, val in seeds:
                key = tuple(seq)
                if key not in uniq or val < uniq[key]:
                    uniq[key] = val
            seeds = sorted(list(uniq.items()), key=lambda kv: kv[1])
            seeds = [(list(k), v) for k, v in seeds[:50]]

            # Dynamically update motifs every few iterations
            if iter_count % 5 == 0:
                new_motifs = self._mine_motifs(seeds, max_len=min(6, self.num_jobs), min_freq=2)
                # merge with previous: keep intersection or strong ones
                combined = []
                seen_m = set()
                for m in new_motifs + motifs:
                    if m not in seen_m:
                        combined.append(m)
                        seen_m.add(m)
                motifs = combined[:min(100, len(combined))]

            # Adapt control parameters
            if no_improve > 10:
                coarse_prob = min(0.9, coarse_prob + 0.05)
                motif_use_prob = max(0.2, motif_use_prob - 0.05)
            else:
                coarse_prob = max(0.3, coarse_prob - 0.01)
                motif_use_prob = min(0.9, motif_use_prob + 0.01)

            # Occasionally perform targeted perturbation/restart seeded from consensus
            if no_improve > 20 or random.random() < perturb_prob:
                # build consensus and perturb it
                consensus = self._build_consensus(seeds)
                seed_from_cons = self._consensus_sequence(consensus, perturb=0.5)
                # apply random block shuffles
                s = seed_from_cons[:]
                for _ in range(max(1, self.num_jobs // 5)):
                    a = random.randrange(self.num_jobs)
                    b = random.randrange(self.num_jobs)
                    if a > b:
                        a, b = b, a
                    s = s[:a] + s[a:b+1][::-1] + s[b+1:]
                s = self._repair_sequence(s)
                s, val = self._local_search_improve(s, time_budget_iters=100)
                seeds.append((s, val))
                no_improve = 0  # reset stagnation

            # update consensus every iteration
            consensus = self._build_consensus(seeds)

        # Post-Processing Phase: final polishing on best_seq
        polished_seq = list(best_seq)
        # run more intensive local search (iterated greedy style)
        polished_val = best_val
        for _ in range(50):
            # random destruction: remove k jobs and reinsert greedily
            k = max(1, int(0.08 * self.num_jobs))
            remove_idx = sorted(random.sample(range(self.num_jobs), k), reverse=True)
            partial = polished_seq[:]
            removed = []
            for idx in remove_idx:
                removed.append(partial.pop(idx))
            # reinsert removed greedily
            for job in removed:
                best_pos = None
                best_val_local = None
                for pos in range(len(partial) + 1):
                    cand = partial[:pos] + [job] + partial[pos:]
                    val = self._makespan(cand)
                    if best_val_local is None or val < best_val_local:
                        best_val_local = val
                        best_pos = pos
                partial = partial[:best_pos] + [job] + partial[best_pos:]
            partial, val = self._local_search_improve(partial, time_budget_iters=200)
            if val < polished_val:
                polished_seq = partial
                polished_val = val

        # Final repair and return
        polished_seq = self._repair_sequence(polished_seq)
        return polished_seq
        

